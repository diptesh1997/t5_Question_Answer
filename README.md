## Generative Domain-based Question-Answer Model & Answer Evaluation System

I worked on this project centered around the development of a **Generative Domain-based Question-Answer Model & Answer Evaluation System** (1). The objective of this endeavor is to streamline the process of generating contextual questions from slides and books, while also creating an answer evaluation system to aid educators in assessing student responses. 

My approach involves leveraging the capabilities of the pre-trained transformer T5-large model, training it on the RACE dataset. The project targets question generation for students enrolled in the *Introduction to Database System* course, necessitating fine-tuning on the database domain. Given the extensive nature of the RACE dataset, which is sourced from English examinations and publicly available, replication within our domain is unfeasible. Therefore, our methodology commences with fine-tuning on the RACE Middle Dataset. Subsequently, we utilize the tokenizer and model parameters from this fine-tuned model to further train it on our custom dataset.

The dataset comprises contexts extracted from research papers concerning database topics as input, alongside contextual question-answer pairs as targets. Our system's frontend processes input in the form of lecture slides or course books. Utilizing Python's pdf reader package, the data preprocessing pipeline extracts the designated page range based on user input. Furthermore, users can specify the desired number of questions. The system generates an equitable distribution of questions from each page, calculated as 'number of questions / number of pages specified'.

For answer evaluation, it's apparent that student responses may not exactly match the content in the slides, but their contextual meaning must align. To address this, we employ the Bert model trained on the STSB (en) & SICK dataset to calculate similarity. Responses exceeding a predefined threshold are considered correct and suitable for human assessment. While the project acknowledges that fully automatic evaluation necessitates advanced technology beyond its scope, the current iteration serves to filter out incorrect answers and provide labeled correct answers for tutor evaluation.
